{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebook For ReadMyMind, A CS 125 @ Illinois MP7 Project by Isaac Park and Mihir Pandya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Avg. Likes</th>\n",
       "      <th>Avg. Retweets</th>\n",
       "      <th>Avg. Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>great</td>\n",
       "      <td>17</td>\n",
       "      <td>74423</td>\n",
       "      <td>15508</td>\n",
       "      <td>0.632480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>america</td>\n",
       "      <td>15</td>\n",
       "      <td>77764</td>\n",
       "      <td>19465</td>\n",
       "      <td>0.631233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fake</td>\n",
       "      <td>12</td>\n",
       "      <td>88798</td>\n",
       "      <td>23246</td>\n",
       "      <td>0.609117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>pensacola</td>\n",
       "      <td>9</td>\n",
       "      <td>61735</td>\n",
       "      <td>13868</td>\n",
       "      <td>0.372297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>cuts</td>\n",
       "      <td>9</td>\n",
       "      <td>72724</td>\n",
       "      <td>16913</td>\n",
       "      <td>0.517917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>tax</td>\n",
       "      <td>9</td>\n",
       "      <td>61127</td>\n",
       "      <td>15095</td>\n",
       "      <td>0.470904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>cnn</td>\n",
       "      <td>7</td>\n",
       "      <td>112497</td>\n",
       "      <td>29027</td>\n",
       "      <td>0.537169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>news</td>\n",
       "      <td>7</td>\n",
       "      <td>97927</td>\n",
       "      <td>25942</td>\n",
       "      <td>0.581112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>market</td>\n",
       "      <td>7</td>\n",
       "      <td>96695</td>\n",
       "      <td>21615</td>\n",
       "      <td>0.517571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bill</td>\n",
       "      <td>6</td>\n",
       "      <td>82313</td>\n",
       "      <td>18484</td>\n",
       "      <td>0.701984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>thank</td>\n",
       "      <td>6</td>\n",
       "      <td>72871</td>\n",
       "      <td>16785</td>\n",
       "      <td>0.600126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>florida</td>\n",
       "      <td>6</td>\n",
       "      <td>67722</td>\n",
       "      <td>15593</td>\n",
       "      <td>0.390126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>honor</td>\n",
       "      <td>5</td>\n",
       "      <td>69315</td>\n",
       "      <td>16056</td>\n",
       "      <td>0.596250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>foxandfriends</td>\n",
       "      <td>5</td>\n",
       "      <td>61884</td>\n",
       "      <td>15179</td>\n",
       "      <td>0.360155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>american</td>\n",
       "      <td>5</td>\n",
       "      <td>68451</td>\n",
       "      <td>17185</td>\n",
       "      <td>0.349571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>pearl</td>\n",
       "      <td>5</td>\n",
       "      <td>72299</td>\n",
       "      <td>16403</td>\n",
       "      <td>0.502500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>big</td>\n",
       "      <td>5</td>\n",
       "      <td>83575</td>\n",
       "      <td>20038</td>\n",
       "      <td>0.498389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>history</td>\n",
       "      <td>5</td>\n",
       "      <td>82737</td>\n",
       "      <td>20390</td>\n",
       "      <td>0.586250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>jones</td>\n",
       "      <td>5</td>\n",
       "      <td>75109</td>\n",
       "      <td>18755</td>\n",
       "      <td>0.493016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>vote</td>\n",
       "      <td>5</td>\n",
       "      <td>71241</td>\n",
       "      <td>16752</td>\n",
       "      <td>0.514556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>christmas</td>\n",
       "      <td>5</td>\n",
       "      <td>93436</td>\n",
       "      <td>25390</td>\n",
       "      <td>0.601643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>rate</td>\n",
       "      <td>4</td>\n",
       "      <td>61656</td>\n",
       "      <td>17022</td>\n",
       "      <td>0.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>whitehouse</td>\n",
       "      <td>4</td>\n",
       "      <td>48685</td>\n",
       "      <td>11062</td>\n",
       "      <td>0.440833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>brian</td>\n",
       "      <td>4</td>\n",
       "      <td>93143</td>\n",
       "      <td>24617</td>\n",
       "      <td>0.527825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>you</td>\n",
       "      <td>4</td>\n",
       "      <td>69390</td>\n",
       "      <td>15278</td>\n",
       "      <td>0.512986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>clinton</td>\n",
       "      <td>4</td>\n",
       "      <td>120158</td>\n",
       "      <td>34569</td>\n",
       "      <td>0.308654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Country</td>\n",
       "      <td>4</td>\n",
       "      <td>102657</td>\n",
       "      <td>27396</td>\n",
       "      <td>0.542882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>god</td>\n",
       "      <td>4</td>\n",
       "      <td>94576</td>\n",
       "      <td>26992</td>\n",
       "      <td>0.706250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>abc</td>\n",
       "      <td>4</td>\n",
       "      <td>110580</td>\n",
       "      <td>31469</td>\n",
       "      <td>0.523785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>economy</td>\n",
       "      <td>4</td>\n",
       "      <td>60006</td>\n",
       "      <td>14320</td>\n",
       "      <td>0.294523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Keywords  Frequency  Avg. Likes  Avg. Retweets  Avg. Sentiment\n",
       "25           great         17       74423          15508        0.632480\n",
       "4          america         15       77764          19465        0.631233\n",
       "2             fake         12       88798          23246        0.609117\n",
       "35       pensacola          9       61735          13868        0.372297\n",
       "209           cuts          9       72724          16913        0.517917\n",
       "208            tax          9       61127          15095        0.470904\n",
       "65             cnn          7      112497          29027        0.537169\n",
       "17            news          7       97927          25942        0.581112\n",
       "11          market          7       96695          21615        0.517571\n",
       "6             bill          6       82313          18484        0.701984\n",
       "52           thank          6       72871          16785        0.600126\n",
       "94         florida          6       67722          15593        0.390126\n",
       "56           honor          5       69315          16056        0.596250\n",
       "169  foxandfriends          5       61884          15179        0.360155\n",
       "153       american          5       68451          17185        0.349571\n",
       "148          pearl          5       72299          16403        0.502500\n",
       "81             big          5       83575          20038        0.498389\n",
       "59         history          5       82737          20390        0.586250\n",
       "86           jones          5       75109          18755        0.493016\n",
       "91            vote          5       71241          16752        0.514556\n",
       "103      christmas          5       93436          25390        0.601643\n",
       "114           rate          4       61656          17022        0.275000\n",
       "139     whitehouse          4       48685          11062        0.440833\n",
       "75           brian          4       93143          24617        0.527825\n",
       "53             you          4       69390          15278        0.512986\n",
       "218        clinton          4      120158          34569        0.308654\n",
       "92         Country          4      102657          27396        0.542882\n",
       "99             god          4       94576          26992        0.706250\n",
       "77             abc          4      110580          31469        0.523785\n",
       "18         economy          4       60006          14320        0.294523"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from datetime import datetime\n",
    "import pprint\n",
    "import numpy as np\n",
    "\n",
    "username = 'realDonaldTrump' # working example: Donald Trump\n",
    "\n",
    "auth = tweepy.OAuthHandler('1X5fCqPl7yVvYxQjQJwkvavFD', 'NXbTDPP3HxlXOL5dWdCegEP09odLAkxUWlyRvZqXxtAtdX597G')\n",
    "auth.set_access_token('925495606931546112-mn3Hda41LsZhbYAKJtddL7TulRKucuj', 'lvCFqSLv5YvOGzCINH6JZ5cBI1CEkPKrRioBn5Iuec3Tt')\n",
    "api = tweepy.API(auth)\n",
    "    \n",
    "tweets_df = pd.DataFrame({\n",
    "    'Timestamp': (),\n",
    "    'Likes': (),\n",
    "    'Retweets': (),\n",
    "    'Text': (),\n",
    "    'Sentences': (),\n",
    "    'Sentiment_Total': (),\n",
    "    'Keywords': ()\n",
    "})\n",
    "\n",
    "tweets_df = tweets_df[['Timestamp', 'Likes', 'Retweets', 'Text', 'Sentences', 'Sentiment_Total', 'Keywords']]\n",
    "    \n",
    "recent_tweets = api.user_timeline(screen_name = username, count=100, tweet_mode=\"extended\") # analyzing 100 tweets\n",
    "for status in recent_tweets:\n",
    "    test = status.full_text\n",
    "    if test[:2] != 'RT': # removing retweets made by the user\n",
    "        status_data = pd.Series([status.created_at, status.favorite_count, status.retweet_count, status.full_text], \n",
    "                                index=['Timestamp', 'Likes', 'Retweets', 'Text'])\n",
    "    tweets_df = tweets_df.append(status_data, ignore_index = True)\n",
    "    \n",
    "tweets_df = tweets_df.drop_duplicates(subset='Text') # just in case, remove any duplicate tweets\n",
    "tweets_df = tweets_df.astype('object')\n",
    "\n",
    "keywords_dict = {}\n",
    "\n",
    "for i in range(len(tweets_df)):\n",
    "    content = tweets_df.iloc[i]['Text']\n",
    "    if 'http' in content:\n",
    "        j = content.index('http')\n",
    "        content = content[:j] # cleaning text of the tweet by removing the link at the end and newline characters\n",
    "    content = content.replace('\\n', '')\n",
    "    tweets_df.iloc[i]['Text'] = content\n",
    "    \n",
    "    blob = TextBlob(content)\n",
    "    tweets_df.iloc[i]['Sentiment_Total'] = blob.sentiment.subjectivity\n",
    "    sentiments = {}\n",
    "    \n",
    "    for sent in blob.sentences: # generating sentiment polarity values for each sentence in the tweet\n",
    "        sentiments[str(sent)] = sent.sentiment.subjectivity\n",
    "        \n",
    "    tweets_df.iloc[i]['Sentences'] = sentiments # insert dictionary of sentence: sentiment value into dataframe\n",
    "    \n",
    "    tweets_df.iloc[i]['Timestamp'] = tweets_df.iloc[i]['Timestamp'].to_pydatetime() # convert pandas.tslib.Timestamp object to datetime\n",
    "    \n",
    "    # Keyword extraction goes here\n",
    "    filtered_words = blob.noun_phrases\n",
    "#     print(filtered_words)\n",
    "    temp = []\n",
    "    \n",
    "    for element in filtered_words:\n",
    "        for x in range(len(filtered_words)):\n",
    "#             print(filtered_words[x])\n",
    "#             print(element)\n",
    "            if element != filtered_words[x] and element in filtered_words[x]:\n",
    "                temp.append(element)\n",
    "                #filtered_words = [x for x in filtered_words if x != element]\n",
    "    parts_of_speech = blob.tags\n",
    "    for element in temp:\n",
    "        filtered_words = [x for x in filtered_words if x != element]\n",
    "    \n",
    "    for x in range(len(parts_of_speech)):\n",
    "        if (parts_of_speech[x])[1] == 'NN':\n",
    "            enter = True\n",
    "            for element in filtered_words:\n",
    "                if (parts_of_speech[x])[0] in element:\n",
    "                    enter = False\n",
    "            if enter:\n",
    "                if x > 0 and (parts_of_speech[x - 1])[1] == 'PRP$':\n",
    "                    filtered_words.append((parts_of_speech[x])[0])\n",
    "    parenthesis = []\n",
    "    paren_init = 0\n",
    "    loc_begin = blob.find(\"(\", paren_init)\n",
    "    loc_end = blob.find(\")\", paren_init)\n",
    "    \n",
    "    while loc_end >= 0:\n",
    "        parenthesis.append(blob[loc_begin:loc_end])\n",
    "        paren_init = loc_end + 1\n",
    "        loc_begin = blob.find(\"(\", paren_init)\n",
    "        loc_end = blob.find(\")\", paren_init)\n",
    "    #print(parenthesis)\n",
    "    \n",
    "    for element in filtered_words:\n",
    "        for pelement in parenthesis:\n",
    "            if element in pelement.lower():\n",
    "                filtered_words = [x for x in filtered_words if x != element]\n",
    "#     tweets_df.iloc[i]['Keywords'] = filtered_words\n",
    "\n",
    "    for word in filtered_words:\n",
    "        separated = TextBlob(word).words\n",
    "        for j in separated:\n",
    "            j = j.strip()\n",
    "            if j.isalpha() and len(j) > 2:\n",
    "                if j in keywords_dict:\n",
    "                    keywords_dict[j][0] += 1\n",
    "                    keywords_dict[j][1] += tweets_df.iloc[i]['Likes']\n",
    "                    keywords_dict[j][2] += tweets_df.iloc[i]['Retweets']\n",
    "                    keywords_dict[j][3] += tweets_df.iloc[i]['Sentiment_Total']\n",
    "#                     sentiment_sum = 0\n",
    "#                     for sent in tweets_df.iloc[i]['Sentences']:\n",
    "#                         if j in sent:\n",
    "#                             sentiment_sum += tweets_df.iloc[i]['Sentences'][sent]\n",
    "#                     keywords_dict[j][3] += sentiment_sum\n",
    "                else:\n",
    "                    keywords_dict[j] = [1, tweets_df.iloc[i]['Likes'], tweets_df.iloc[i]['Retweets'], tweets_df.iloc[i]['Sentiment_Total']]\n",
    "#                     sentiment_sum = 0\n",
    "#                     for sent in tweets_df.iloc[i]['Sentences']:\n",
    "#                         if j in sent:\n",
    "#                             sentiment_sum += tweets_df.iloc[i]['Sentences'][sent]\n",
    "#                     keywords_dict[j][3] = sentiment_sum\n",
    "\n",
    "for key in keywords_dict:\n",
    "    keywords_dict[key][1] = int(keywords_dict[key][1] / keywords_dict[key][0])\n",
    "    keywords_dict[key][2] = int(keywords_dict[key][2] / keywords_dict[key][0])\n",
    "    keywords_dict[key][3] = keywords_dict[key][3] / keywords_dict[key][0]\n",
    "\n",
    "keywords_df = pd.DataFrame.from_dict(keywords_dict, orient='index')\n",
    "keywords_df.columns = ['Frequency', 'Avg. Likes', 'Avg. Retweets', 'Avg. Sentiment']\n",
    "keywords_df.index.name = 'Keywords'\n",
    "keywords_df.reset_index(inplace = True)\n",
    "keywords_df = keywords_df.sort_values(['Frequency'], ascending = [False], na_position = 'last')\n",
    "keywords_df = keywords_df[:30]\n",
    "\n",
    "keywords_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Proposed method for keyword extraction:\n",
    "\n",
    "1. Tokenize each word with part of speech tag. keep only proper nouns, nouns, adjectives, and verbs.\n",
    "2. Score the nouns and proper nouns based on amount of surrounding adjectives and verbs (using more description tends to indicate importance).\n",
    "3. Record frequency of each word; only keep words that occur above a certain number of times (frequency threshold). These will be our \"keywords\".\n",
    "4. Put the list of keywords for each tweet into the 'Keywords' column of the dataframe.\n",
    "\n",
    "Ideas for graphing the keywords/frequency/likes/retweets relationships:\n",
    "\n",
    "1. y-axis: frequency, x-axis: keyword; simple bar graph of the top keywords\n",
    "\n",
    "2. y-axis: likes/retweet count, x-axis: frequencies of keywords; scatter plot with each dot representing a keyword.\n",
    "\n",
    "3. Simple pie chart to analyze the main content areas that said Twitter account comments on.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written below is the function/method oriented version of the program. Different parts of the program are organized into multiple functions. -Isaac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['democratic party', 'state party', 'young people', 'unity reform', 'country']\n",
      "['republican', 'tax bill', 'moral abomination', 'taxdebate']\n",
      "['trans people', 'tdor', 'society']\n",
      "['disaster relief efforts', 'iran', 'important act']\n",
      "['iran']\n",
      "['progressives', 'didn ’ t', 'trump', 'country work']\n",
      "['trump', 'republican', 'american people', 'democratic party']\n",
      "['local positions', 'political revolution']\n",
      "['political revolution', 'political process']\n",
      "['first-time candidates', 'state legislatures', 'city councils', 'school boards']\n",
      "['yesterday', 'trump', '’ s', 'american people', 'country']\n",
      "['’ s election day', 'nyc', 'billdeblasio']\n",
      "['’ s #', 'electionday', 'jersey', 'democratic ticket']\n",
      "['’ s time', '@ fortforatlanta', 'electionday find']\n",
      "['’ s #', 'electionday', 'democratic ticket', 'vote counts']\n",
      "['’ s election day', 'virginia', 'democratic ticket', 'find']\n",
      "['sutherland', 'senseless tragedy']\n",
      "['wall street', 'public colleges', 'universities tuition-free', 'street speculators']\n",
      "['corporate tax evaders', 'nation ’ s infrastructure', 'offshore tax loopholes']\n",
      "['wealthiest people', 'fair share']\n",
      "['’ s horrific attack', 'manhattan']\n",
      "['billdeblasio']\n",
      "['u.s.', 'reckless president']\n",
      "['trump', 'puerto rico', 'congress', 'disaster relief package']\n",
      "[\"ca n't\", 'birth control', 'women ’ s rights']\n",
      "['trump']\n",
      "['inadequate federal response', 'hurricane maria']\n",
      "['american citizens', 'puerto ricans', 'texas', 'florida', 'natural disasters']\n",
      "['republican', 'budget ignores']\n",
      "['congratulations', 'mayor-elect', 'woodfinforbham', 'great grassroots']\n",
      "['woodfinforbham', 'real progressive', 'voice']\n",
      "['politicians', 'gerrymander districts', 'political advantages', 'democracy']\n",
      "['innocent lives']\n",
      "['mass shootings', 'long overdue', 'congress', 'gun safety']\n",
      "['las vegas']\n",
      "['medicare', 'trump', 'huge tax']\n",
      "['massive inequality', 'republican', 'middle class', 'huge tax']\n",
      "['republican', 'disastrous “ health care', 'heavy political price']\n",
      "['flood', 'republican']\n",
      "['mccain', '’ s opposition', 'republican', 'health care bill', 'own opposition']\n",
      "['republican', 'grahamcassidy', 'health care bill']\n",
      "['health care', 'current system', 'job']\n",
      "['health care system', 'medicareforall']\n",
      "['great nation', 'compassion']\n",
      "['major country', 'doesn ’ t', 'health care', 'medicareforall']\n",
      "['universal', 'health care', 'u.s.', 'medicareforall']\n",
      "['’ s', 'financial disaster', 'medicareforall']\n",
      "['universal health care']\n",
      "['health care']\n",
      "['thank', '@ tammybaldwin', 'medicare']\n",
      "['@ elizabethforma', 'thank', '@ elizabethforma', 'health care', 'side']\n",
      "['years unions', 'equal rights', 'social justice']\n",
      "['white supremacists']\n",
      "['immigrant families', 'young people', 'daca', 'america', 'home']\n",
      "['hurricane harvey']\n",
      "['arpaio', 'trump']\n",
      "['portsmouth', 'ohio', 'opioid crisis', 'join']\n",
      "['live', 'join bernie', 'town hall', 'ohio', 'health care']\n",
      "['republican', 'health care plan']\n",
      "['watch live', 'bernie', 'goodjobsnation', 'indianapolis', 'trump']\n",
      "['indiana', 'michigan', 'ohio', 'simple point', 'all', 'health care']\n",
      "['join', 'chuck jones', 'goodjobsnation', 'indianapolis', 'good jobs', 'livable wages']\n",
      "['global community']\n",
      "['barcelona', 'horrific terror attack']\n",
      "['immigrants', 'rational path', 'dark corner', 'defenddaca']\n",
      "['memory']\n",
      "['heather', 'racial justice']\n",
      "['heather heyer', 'neo-nazism', 'white supremacy']\n",
      "['minorities surge']\n",
      "['charlottesville']\n",
      "['white nationalist demonstration', 'charlottesville', 'reprehensible display', 'society']\n",
      "['bottom', 'quality health care']\n",
      "['health insurance', 'think']\n",
      "[\"'health care\", 'destructive legislation', 'modern history']\n",
      "['john mccain', 'family']\n",
      "['health insurance', 'health care']\n",
      "['great victory', 'hard work']\n",
      "['great nation isn ’ t']\n",
      "['benjealous ben', 'progressive agenda', 'political spectrum', 'md']\n",
      "['benjealous', 'important thing', 'trump', '’ s destructive agenda', 'strong progressive leaders']\n",
      "['benjealous', 'maryland', '’ s', 'courageous activism']\n",
      "['thanks', 'obamacare', 'william', '’ t', 'health insurance']\n",
      "['republican', 'health care bill', 'american people']\n",
      "['’ s', 'senatemajldr']\n",
      "['senatemajldr', 'own state']\n",
      "['kentucky', 'mitch mcconnell', 'health care', 'tax cuts']\n",
      "['great nation', 'number']\n",
      "['virginia', 'don ’ t', 'health care', 'billionaires tax']\n",
      "['virginia', '’ s overdose death rate', 'u.s.', '$ 800b', 'medicaid']\n",
      "[]\n",
      "['small tweak', 'massive damage', 'virginia']\n",
      "['such enormous magnitude', 'public comment', 'health care']\n",
      "['sencapito', 'america', '’ re', 'vulnerable people', 'massive tax']\n",
      "['middle class']\n",
      "['join', 'capito', 'health insurance']\n",
      "['join', 'morgantown', 'virginia', 'health care']\n",
      "['thank', 'donnafedwards', 'america', 'story']\n",
      "['senatemajldr', 'bottom line', 'america', 'kentucky', 'trump']\n",
      "['senatemajldr', 'mcconnell', \"'s legislation\", 'kentuckians', 'opioid addiction']\n",
      "['unbelievably', 'kentucky', 'huge progress', 'health care', 'senatemajldr', 'kentuckians', 'health insurance']\n",
      "['join', 'kentucky', 'health care', 'voice']\n",
      "['republican', 'senator vote', 'care']\n",
      "['july']\n",
      "['independence', 'read', 'declaration', 'independence']\n",
      "['health care', 'open debate']\n",
      "['mcconnell', 'health care bill']\n",
      "['remember', 'wealthiest country', 'health care']\n",
      "[\"ca n't\", 'health care']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Avg. Likes</th>\n",
       "      <th>Avg. Retweets</th>\n",
       "      <th>Avg. Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>health</td>\n",
       "      <td>31</td>\n",
       "      <td>22410</td>\n",
       "      <td>6364</td>\n",
       "      <td>0.383967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>care</td>\n",
       "      <td>28</td>\n",
       "      <td>24019</td>\n",
       "      <td>6786</td>\n",
       "      <td>0.394366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>republican</td>\n",
       "      <td>11</td>\n",
       "      <td>17020</td>\n",
       "      <td>4451</td>\n",
       "      <td>0.505177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>trump</td>\n",
       "      <td>10</td>\n",
       "      <td>28899</td>\n",
       "      <td>8761</td>\n",
       "      <td>0.511472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tax</td>\n",
       "      <td>8</td>\n",
       "      <td>9221</td>\n",
       "      <td>3110</td>\n",
       "      <td>0.411979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>people</td>\n",
       "      <td>8</td>\n",
       "      <td>22943</td>\n",
       "      <td>5090</td>\n",
       "      <td>0.394444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>political</td>\n",
       "      <td>6</td>\n",
       "      <td>13626</td>\n",
       "      <td>2896</td>\n",
       "      <td>0.320833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>join</td>\n",
       "      <td>6</td>\n",
       "      <td>2273</td>\n",
       "      <td>566</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>virginia</td>\n",
       "      <td>5</td>\n",
       "      <td>7035</td>\n",
       "      <td>1979</td>\n",
       "      <td>0.296667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>country</td>\n",
       "      <td>5</td>\n",
       "      <td>27450</td>\n",
       "      <td>6395</td>\n",
       "      <td>0.425556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>great</td>\n",
       "      <td>5</td>\n",
       "      <td>23564</td>\n",
       "      <td>6466</td>\n",
       "      <td>0.615278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>insurance</td>\n",
       "      <td>5</td>\n",
       "      <td>14679</td>\n",
       "      <td>4269</td>\n",
       "      <td>0.337143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bill</td>\n",
       "      <td>5</td>\n",
       "      <td>23487</td>\n",
       "      <td>7261</td>\n",
       "      <td>0.463333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>senatemajldr</td>\n",
       "      <td>5</td>\n",
       "      <td>4155</td>\n",
       "      <td>1239</td>\n",
       "      <td>0.636667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>democratic</td>\n",
       "      <td>5</td>\n",
       "      <td>8877</td>\n",
       "      <td>1853</td>\n",
       "      <td>0.283510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>medicareforall</td>\n",
       "      <td>4</td>\n",
       "      <td>9292</td>\n",
       "      <td>2726</td>\n",
       "      <td>0.328125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>nation</td>\n",
       "      <td>4</td>\n",
       "      <td>22443</td>\n",
       "      <td>6707</td>\n",
       "      <td>0.536458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>american</td>\n",
       "      <td>4</td>\n",
       "      <td>28972</td>\n",
       "      <td>6452</td>\n",
       "      <td>0.236806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>america</td>\n",
       "      <td>4</td>\n",
       "      <td>6862</td>\n",
       "      <td>2068</td>\n",
       "      <td>0.538542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>kentucky</td>\n",
       "      <td>4</td>\n",
       "      <td>4149</td>\n",
       "      <td>1242</td>\n",
       "      <td>0.346875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>progressive</td>\n",
       "      <td>3</td>\n",
       "      <td>1702</td>\n",
       "      <td>415</td>\n",
       "      <td>0.415556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>thank</td>\n",
       "      <td>3</td>\n",
       "      <td>4439</td>\n",
       "      <td>895</td>\n",
       "      <td>0.728571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>electionday</td>\n",
       "      <td>3</td>\n",
       "      <td>1902</td>\n",
       "      <td>626</td>\n",
       "      <td>0.096591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>mcconnell</td>\n",
       "      <td>3</td>\n",
       "      <td>11384</td>\n",
       "      <td>3797</td>\n",
       "      <td>0.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>massive</td>\n",
       "      <td>3</td>\n",
       "      <td>2662</td>\n",
       "      <td>899</td>\n",
       "      <td>0.654167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>disaster</td>\n",
       "      <td>3</td>\n",
       "      <td>18050</td>\n",
       "      <td>4082</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>huge</td>\n",
       "      <td>3</td>\n",
       "      <td>8998</td>\n",
       "      <td>3908</td>\n",
       "      <td>0.754167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>party</td>\n",
       "      <td>3</td>\n",
       "      <td>21526</td>\n",
       "      <td>3956</td>\n",
       "      <td>0.581481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>state</td>\n",
       "      <td>3</td>\n",
       "      <td>14880</td>\n",
       "      <td>2597</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>benjealous</td>\n",
       "      <td>3</td>\n",
       "      <td>4742</td>\n",
       "      <td>1067</td>\n",
       "      <td>0.648889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Keywords  Frequency  Avg. Likes  Avg. Retweets  Avg. Sentiment\n",
       "126          health         31       22410           6364        0.383967\n",
       "127            care         28       24019           6786        0.394366\n",
       "8        republican         11       17020           4451        0.505177\n",
       "25            trump         10       28899           8761        0.511472\n",
       "9               tax          8        9221           3110        0.411979\n",
       "4            people          8       22943           5090        0.394444\n",
       "30        political          6       13626           2896        0.320833\n",
       "166            join          6        2273            566        0.266667\n",
       "52         virginia          5        7035           1979        0.296667\n",
       "7           country          5       27450           6395        0.425556\n",
       "99            great          5       23564           6466        0.615278\n",
       "210       insurance          5       14679           4269        0.337143\n",
       "10             bill          5       23487           7261        0.463333\n",
       "234    senatemajldr          5        4155           1239        0.636667\n",
       "0        democratic          5        8877           1853        0.283510\n",
       "138  medicareforall          4        9292           2726        0.328125\n",
       "64           nation          4       22443           6707        0.536458\n",
       "27         american          4       28972           6452        0.236806\n",
       "158         america          4        6862           2068        0.538542\n",
       "235        kentucky          4        4149           1242        0.346875\n",
       "102     progressive          3        1702            415        0.415556\n",
       "144           thank          3        4439            895        0.728571\n",
       "44      electionday          3        1902            626        0.096591\n",
       "237       mcconnell          3       11384           3797        0.187500\n",
       "121         massive          3        2662            899        0.654167\n",
       "17         disaster          3       18050           4082        0.583333\n",
       "120            huge          3        8998           3908        0.754167\n",
       "1             party          3       21526           3956        0.581481\n",
       "2             state          3       14880           2597        0.533333\n",
       "221      benjealous          3        4742           1067        0.648889"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "username = 'realDonaldTrump' # working example: Donald Trump\n",
    "auth = tweepy.OAuthHandler('1X5fCqPl7yVvYxQjQJwkvavFD', 'NXbTDPP3HxlXOL5dWdCegEP09odLAkxUWlyRvZqXxtAtdX597G')\n",
    "auth.set_access_token('925495606931546112-mn3Hda41LsZhbYAKJtddL7TulRKucuj', 'lvCFqSLv5YvOGzCINH6JZ5cBI1CEkPKrRioBn5Iuec3Tt')\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "def tweet_collector(username): \n",
    "    tweets_df = pd.DataFrame({\n",
    "        'Timestamp': (),\n",
    "        'Likes': (),\n",
    "        'Retweets': (),\n",
    "        'Text': (),\n",
    "        'Sentences': (),\n",
    "        'Sentiment_Total': (),\n",
    "        'Keywords': ()\n",
    "    })\n",
    "\n",
    "    tweets_df = tweets_df[['Timestamp', 'Likes', 'Retweets', 'Text', 'Sentences', 'Sentiment_Total', 'Keywords']]\n",
    "\n",
    "    recent_tweets = api.user_timeline(screen_name = username, count=100, tweet_mode=\"extended\") # analyzing 100 tweets\n",
    "    for status in recent_tweets:\n",
    "        test = status.full_text\n",
    "        if test[:2] != 'RT': # removing retweets made by the user\n",
    "            status_data = pd.Series([status.created_at, status.favorite_count, status.retweet_count, status.full_text], \n",
    "                                    index=['Timestamp', 'Likes', 'Retweets', 'Text'])\n",
    "        tweets_df = tweets_df.append(status_data, ignore_index = True)\n",
    "\n",
    "    tweets_df = tweets_df.drop_duplicates(subset='Text') # just in case, remove any duplicate tweets\n",
    "    tweets_df = tweets_df.astype('object')\n",
    "    \n",
    "    return tweets_df\n",
    "\n",
    "def keyword_data(tweets_df):\n",
    "    keywords_dict = {}\n",
    "\n",
    "    for i in range(len(tweets_df)):\n",
    "        content = tweets_df.iloc[i]['Text']\n",
    "        if 'http' in content:\n",
    "            j = content.index('http')\n",
    "            content = content[:j] # cleaning text of the tweet by removing the link at the end and newline characters\n",
    "        content = content.replace('\\n', '')\n",
    "        tweets_df.iloc[i]['Text'] = content\n",
    "\n",
    "        blob = TextBlob(content)\n",
    "        tweets_df.iloc[i]['Sentiment_Total'] = blob.sentiment.subjectivity\n",
    "        sentiments = {}\n",
    "\n",
    "        for sent in blob.sentences: # generating sentiment polarity values for each sentence in the tweet\n",
    "            sentiments[str(sent)] = sent.sentiment.subjectivity\n",
    "\n",
    "        tweets_df.iloc[i]['Sentences'] = sentiments # insert dictionary of sentence: sentiment value into dataframe\n",
    "\n",
    "        tweets_df.iloc[i]['Timestamp'] = tweets_df.iloc[i]['Timestamp'].to_pydatetime() # convert pandas.tslib.Timestamp object to datetime\n",
    "\n",
    "        # Keyword extraction goes here\n",
    "        filtered_words = blob.noun_phrases\n",
    "    #     print(filtered_words)\n",
    "        temp = []\n",
    "\n",
    "        for element in filtered_words:\n",
    "            for x in range(len(filtered_words)):\n",
    "    #             print(filtered_words[x])\n",
    "    #             print(element)\n",
    "                if element != filtered_words[x] and element in filtered_words[x]:\n",
    "                    temp.append(element)\n",
    "                    #filtered_words = [x for x in filtered_words if x != element]\n",
    "        parts_of_speech = blob.tags\n",
    "        for element in temp:\n",
    "            filtered_words = [x for x in filtered_words if x != element]\n",
    "\n",
    "        for x in range(len(parts_of_speech)):\n",
    "            if (parts_of_speech[x])[1] == 'NN':\n",
    "                enter = True\n",
    "                for element in filtered_words:\n",
    "                    if (parts_of_speech[x])[0] in element:\n",
    "                        enter = False\n",
    "                if enter:\n",
    "                    if x > 0 and (parts_of_speech[x - 1])[1] == 'PRP$':\n",
    "                        filtered_words.append((parts_of_speech[x])[0])\n",
    "        parenthesis = []\n",
    "        paren_init = 0\n",
    "        loc_begin = blob.find(\"(\", paren_init)\n",
    "        loc_end = blob.find(\")\", paren_init)\n",
    "\n",
    "        while loc_end >= 0:\n",
    "            parenthesis.append(blob[loc_begin:loc_end])\n",
    "            paren_init = loc_end + 1\n",
    "            loc_begin = blob.find(\"(\", paren_init)\n",
    "            loc_end = blob.find(\")\", paren_init)\n",
    "        #print(parenthesis)\n",
    "\n",
    "        for element in filtered_words:\n",
    "            for pelement in parenthesis:\n",
    "                if element in pelement.lower():\n",
    "                    filtered_words = [x for x in filtered_words if x != element]\n",
    "    #     tweets_df.iloc[i]['Keywords'] = filtered_words\n",
    "\n",
    "        for word in filtered_words:\n",
    "            separated = TextBlob(word).words\n",
    "            for j in separated:\n",
    "                j = j.strip()\n",
    "                if j.isalpha() and len(j) > 2:\n",
    "                    if j in keywords_dict:\n",
    "                        keywords_dict[j][0] += 1\n",
    "                        keywords_dict[j][1] += tweets_df.iloc[i]['Likes']\n",
    "                        keywords_dict[j][2] += tweets_df.iloc[i]['Retweets']\n",
    "                        keywords_dict[j][3] += tweets_df.iloc[i]['Sentiment_Total']\n",
    "                    else:\n",
    "                        keywords_dict[j] = [1, tweets_df.iloc[i]['Likes'], tweets_df.iloc[i]['Retweets'], tweets_df.iloc[i]['Sentiment_Total']]\n",
    "\n",
    "    for key in keywords_dict:\n",
    "        keywords_dict[key][1] = int(keywords_dict[key][1] / keywords_dict[key][0])\n",
    "        keywords_dict[key][2] = int(keywords_dict[key][2] / keywords_dict[key][0])\n",
    "        keywords_dict[key][3] = keywords_dict[key][3] / keywords_dict[key][0]\n",
    "        \n",
    "    return keywords_dict\n",
    "\n",
    "def to_dataframe(keywords_dict):\n",
    "    keywords_df = pd.DataFrame.from_dict(keywords_dict, orient='index')\n",
    "    keywords_df.columns = ['Frequency', 'Avg. Likes', 'Avg. Retweets', 'Avg. Sentiment']\n",
    "    keywords_df.index.name = 'Keywords'\n",
    "    keywords_df.reset_index(inplace = True)\n",
    "    keywords_df = keywords_df.sort_values(['Frequency'], ascending = [False], na_position = 'last')\n",
    "    keywords_df = keywords_df[:30]\n",
    "    \n",
    "    return keywords_df\n",
    "\n",
    "data_df = tweet_collector(username)\n",
    "keywords = keyword_data(data_df)\n",
    "key_df = to_dataframe(keywords)\n",
    "\n",
    "key_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
