{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebook For ReadMyMind, A CS 125 @ Illinois MP7 Project by Isaac Park and Mihir Pandya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>great</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>s</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>america</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fake</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>tax</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>cuts</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>pensacola</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>market</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>cnn</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>news</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>florida</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bill</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>thank</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>honor</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>history</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>foxandfriends</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>vote</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>pearl</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>christmas</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>american</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>jones</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>t</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>big</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>clinton</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>whitehouse</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>brian</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Country</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>rate</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>massive</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>record</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>jackson</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>responders</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>andrew</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>book</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>new</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>december</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>warriors</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>pauses</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>flies</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>officials</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>tickets</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>immigration</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>scholar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>ever</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>highest</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>putting</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>enjoy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>political</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>dershowitz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>alan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>legal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>yesterday</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>escalante</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>staircase</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>ears</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>designations</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>monuments</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>proclamations</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>magnificent</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>winner</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>437 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Keywords  Frequency\n",
       "25           great         17\n",
       "28               s         17\n",
       "4          america         15\n",
       "2             fake         12\n",
       "212            tax          9\n",
       "213           cuts          9\n",
       "36       pensacola          9\n",
       "11          market          7\n",
       "67             cnn          7\n",
       "17            news          7\n",
       "98         florida          6\n",
       "6             bill          6\n",
       "54           thank          6\n",
       "58           honor          5\n",
       "61         history          5\n",
       "173  foxandfriends          5\n",
       "95            vote          5\n",
       "152          pearl          5\n",
       "107      christmas          5\n",
       "157       american          5\n",
       "90           jones          5\n",
       "88               t          5\n",
       "84             big          5\n",
       "222        clinton          4\n",
       "143     whitehouse          4\n",
       "78           brian          4\n",
       "96         Country          4\n",
       "118           rate          4\n",
       "211        massive          4\n",
       "12          record          4\n",
       "..             ...        ...\n",
       "171        jackson          1\n",
       "184     responders          1\n",
       "170         andrew          1\n",
       "169           book          1\n",
       "168            new          1\n",
       "167       december          1\n",
       "165       warriors          1\n",
       "163         pauses          1\n",
       "161          flies          1\n",
       "183      officials          1\n",
       "186        tickets          1\n",
       "215    immigration          1\n",
       "197        scholar          1\n",
       "209           ever          1\n",
       "208        highest          1\n",
       "206        putting          1\n",
       "203          enjoy          1\n",
       "202      political          1\n",
       "199     dershowitz          1\n",
       "198           alan          1\n",
       "196          legal          1\n",
       "187      yesterday          1\n",
       "195      escalante          1\n",
       "194      staircase          1\n",
       "193           ears          1\n",
       "192   designations          1\n",
       "191      monuments          1\n",
       "189  proclamations          1\n",
       "188    magnificent          1\n",
       "436         winner          1\n",
       "\n",
       "[437 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from datetime import datetime\n",
    "import pprint\n",
    "\n",
    "username = 'realDonaldTrump' # working example: Donald Trump\n",
    "\n",
    "auth = tweepy.OAuthHandler('1X5fCqPl7yVvYxQjQJwkvavFD', 'NXbTDPP3HxlXOL5dWdCegEP09odLAkxUWlyRvZqXxtAtdX597G')\n",
    "auth.set_access_token('925495606931546112-mn3Hda41LsZhbYAKJtddL7TulRKucuj', 'lvCFqSLv5YvOGzCINH6JZ5cBI1CEkPKrRioBn5Iuec3Tt')\n",
    "api = tweepy.API(auth)\n",
    "    \n",
    "tweets_df = pd.DataFrame({\n",
    "    'Timestamp': (),\n",
    "    'Likes': (),\n",
    "    'Retweets': (),\n",
    "    'Text': (),\n",
    "    'Sentences': (),\n",
    "    'Sentiment_Total': (),\n",
    "    'Keywords': ()\n",
    "})\n",
    "\n",
    "tweets_df = tweets_df[['Timestamp', 'Likes', 'Retweets', 'Text', 'Sentences', 'Sentiment_Total', 'Keywords']]\n",
    "    \n",
    "recent_tweets = api.user_timeline(screen_name = username, count=100, tweet_mode=\"extended\") # analyzing 100 tweets\n",
    "for status in recent_tweets:\n",
    "    test = status.full_text\n",
    "    if test[:2] != 'RT': # removing retweets made by the user\n",
    "        status_data = pd.Series([status.created_at, status.favorite_count, status.retweet_count, status.full_text], \n",
    "                                index=['Timestamp', 'Likes', 'Retweets', 'Text'])\n",
    "    tweets_df = tweets_df.append(status_data, ignore_index = True)\n",
    "    \n",
    "tweets_df = tweets_df.drop_duplicates(subset='Text') # just in case, remove any duplicate tweets\n",
    "tweets_df = tweets_df.astype('object')\n",
    "\n",
    "keywords_dict = {}\n",
    "\n",
    "for i in range(len(tweets_df)):\n",
    "    content = tweets_df.iloc[i]['Text']\n",
    "    if 'http' in content:\n",
    "        j = content.index('http')\n",
    "        content = content[:j] # cleaning text of the tweet by removing the link at the end and newline characters\n",
    "    content = content.replace('\\n', '')\n",
    "    tweets_df.iloc[i]['Text'] = content\n",
    "    \n",
    "    blob = TextBlob(content)\n",
    "    tweets_df.iloc[i]['Sentiment_Total'] = blob.sentiment.subjectivity\n",
    "    sentiments = {}\n",
    "    \n",
    "    for sent in blob.sentences: # generating sentiment polarity values for each sentence in the tweet\n",
    "        sentiments[str(sent)] = sent.sentiment.subjectivity\n",
    "        \n",
    "    tweets_df.iloc[i]['Sentences'] = sentiments # insert dictionary of sentence: sentiment value into dataframe\n",
    "    \n",
    "    tweets_df.iloc[i]['Timestamp'] = tweets_df.iloc[i]['Timestamp'].to_pydatetime() # convert pandas.tslib.Timestamp object to datetime\n",
    "    \n",
    "    # Keyword extraction goes here\n",
    "    filtered_words = blob.noun_phrases\n",
    "#     print(filtered_words)\n",
    "    temp = []\n",
    "    \n",
    "    for element in filtered_words:\n",
    "        for x in range(len(filtered_words)):\n",
    "#             print(filtered_words[x])\n",
    "#             print(element)\n",
    "            if element != filtered_words[x] and element in filtered_words[x]:\n",
    "                temp.append(element)\n",
    "                #filtered_words = [x for x in filtered_words if x != element]\n",
    "    parts_of_speech = blob.tags\n",
    "    for element in temp:\n",
    "        filtered_words = [x for x in filtered_words if x != element]\n",
    "    \n",
    "    for x in range(len(parts_of_speech)):\n",
    "        if (parts_of_speech[x])[1] == 'NN':\n",
    "            enter = True\n",
    "            for element in filtered_words:\n",
    "                if (parts_of_speech[x])[0] in element:\n",
    "                    enter = False\n",
    "            if enter:\n",
    "                if x > 0 and (parts_of_speech[x - 1])[1] == 'PRP$':\n",
    "                    filtered_words.append((parts_of_speech[x])[0])\n",
    "    parenthesis = []\n",
    "    paren_init = 0\n",
    "    loc_begin = blob.find(\"(\", paren_init)\n",
    "    loc_end = blob.find(\")\", paren_init)\n",
    "    \n",
    "    while loc_end >= 0:\n",
    "        parenthesis.append(blob[loc_begin:loc_end])\n",
    "        paren_init = loc_end + 1\n",
    "        loc_begin = blob.find(\"(\", paren_init)\n",
    "        loc_end = blob.find(\")\", paren_init)\n",
    "    #print(parenthesis)\n",
    "    \n",
    "    for element in filtered_words:\n",
    "        for pelement in parenthesis:\n",
    "            if element in pelement.lower():\n",
    "                filtered_words = [x for x in filtered_words if x != element]\n",
    "    tweets_df.iloc[i]['Keywords'] = filtered_words\n",
    "\n",
    "    for word in filtered_words:\n",
    "        separated = TextBlob(word).words\n",
    "        for j in separated:\n",
    "            if j.isalpha():\n",
    "                if j in keywords_dict:\n",
    "                    keywords_dict[j] += 1\n",
    "                else:\n",
    "                    keywords_dict[j] = 1\n",
    "    \n",
    "keywords_df = pd.DataFrame.from_dict(keywords_dict, orient='index')\n",
    "keywords_df.columns = ['Frequency']\n",
    "keywords_df.index.name = 'Keywords'\n",
    "keywords_df.reset_index(inplace = True)\n",
    "keywords_df = keywords_df.sort_values(['Frequency'], ascending = [False], na_position = 'last')\n",
    "            \n",
    "tweets_df\n",
    "keywords_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Proposed method for keyword extraction:\n",
    "\n",
    "1. Tokenize each word with part of speech tag. keep only proper nouns, nouns, adjectives, and verbs.\n",
    "2. Score the nouns and proper nouns based on amount of surrounding adjectives and verbs (using more description tends to indicate importance).\n",
    "3. Record frequency of each word; only keep words that occur above a certain number of times (frequency threshold). These will be our \"keywords\".\n",
    "4. Put the list of keywords for each tweet into the 'Keywords' column of the dataframe.\n",
    "\n",
    "Ideas for graphing the keywords/frequency/likes/retweets relationships:\n",
    "\n",
    "1. y-axis: frequencies of keywords over time, x-axis: time; line graph with each line representing the popularity of one keyword over time.\n",
    "\n",
    "2. y-axis: likes/retweet count, x-axis: frequencies of keywords; scatter plot with each dot representing a keyword.\n",
    "\n",
    "3. Simple pie chart to analyze the main content areas that said Twitter account comments on.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
