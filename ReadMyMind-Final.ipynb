{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebook For ReadMyMind, A CS 125 @ Illinois MP7 Project by Isaac Park and Mihir Pandya.\n",
    "\n",
    "Github: https://github.com/ReadMyMind-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Proposed method for keyword extraction:\n",
    "\n",
    "1. Tokenize each word with part of speech tag. keep only proper nouns, nouns, adjectives, and verbs.\n",
    "2. Score the nouns and proper nouns based on amount of surrounding adjectives and verbs (using more description tends to indicate importance).\n",
    "3. Record frequency of each word; only keep words that occur above a certain number of times (frequency threshold). These will be our \"keywords\".\n",
    "4. Put the list of keywords for each tweet into the 'Keywords' column of the dataframe.\n",
    "\n",
    "Ideas for graphing the keywords/frequency/likes/retweets relationships:\n",
    "\n",
    "1. Bar graph whose y-axis is the avg. # of likes for a given keyword and x-axis is the list of the top ~30 most occurring keywords.\n",
    "\n",
    "2. Bar graph whose y-axis is the avg. sentiment value for a given keyword and x-axis is the list of the top ~30 most occurring keywords.\n",
    "\n",
    "3. Simple pie chart to analyze the main content areas that said Twitter account comments on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written below is the function/method oriented version of the program. Different parts of the program are organized into multiple functions.\n",
    "\n",
    "Isaac - Twitter Data Collection & Organization, Visualizations\n",
    "\n",
    "Mihir - Keyword Extraction & Refinement, Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from datetime import datetime\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "plotly.tools.set_credentials_file(username='Parkkeo1', api_key='qNMv1LXTLzwfGZ3EyN0U')\n",
    "\n",
    "auth = tweepy.OAuthHandler('1X5fCqPl7yVvYxQjQJwkvavFD', 'NXbTDPP3HxlXOL5dWdCegEP09odLAkxUWlyRvZqXxtAtdX597G')\n",
    "auth.set_access_token('925495606931546112-mn3Hda41LsZhbYAKJtddL7TulRKucuj', 'lvCFqSLv5YvOGzCINH6JZ5cBI1CEkPKrRioBn5Iuec3Tt')\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "\n",
    "def tweet_collector(username): \n",
    "    tweets_df = pd.DataFrame({\n",
    "        'Timestamp': (),\n",
    "        'Likes': (),\n",
    "        'Retweets': (),\n",
    "        'Text': (),\n",
    "        'Sentences': (),\n",
    "        'Sentiment_Total': (),\n",
    "        'Keywords': ()\n",
    "    })\n",
    "\n",
    "    tweets_df = tweets_df[['Timestamp', 'Likes', 'Retweets', 'Text', 'Sentences', 'Sentiment_Total', 'Keywords']]\n",
    "\n",
    "    recent_tweets = api.user_timeline(screen_name = username, count=500, tweet_mode=\"extended\") # analyzing 500 tweets\n",
    "    for status in recent_tweets:\n",
    "        test = status.full_text\n",
    "        if test[:2] != 'RT': # removing retweets made by the user\n",
    "            status_data = pd.Series([status.created_at, status.favorite_count, status.retweet_count, status.full_text], \n",
    "                                    index=['Timestamp', 'Likes', 'Retweets', 'Text'])\n",
    "        tweets_df = tweets_df.append(status_data, ignore_index = True)\n",
    "\n",
    "    tweets_df = tweets_df.drop_duplicates(subset='Text') # just in case, remove any duplicate tweets\n",
    "    tweets_df = tweets_df.astype('object')\n",
    "    \n",
    "    return tweets_df\n",
    "\n",
    "\n",
    "def keyword_data(tweets_df):\n",
    "    keywords_dict = {}\n",
    "\n",
    "    for i in range(len(tweets_df)):\n",
    "        content = tweets_df.iloc[i]['Text']\n",
    "        if 'http' in content:\n",
    "            j = content.index('http')\n",
    "            content = content[:j] # cleaning text of the tweet by removing the link at the end and newline characters\n",
    "        content = content.replace('\\n', '')\n",
    "        tweets_df.iloc[i]['Text'] = content\n",
    "\n",
    "        blob = TextBlob(content)\n",
    "        tweets_df.iloc[i]['Sentiment_Total'] = blob.sentiment.subjectivity\n",
    "        sentiments = {}\n",
    "\n",
    "        for sent in blob.sentences: # generating sentiment polarity values for each sentence in the tweet\n",
    "            sentiments[str(sent)] = sent.sentiment.subjectivity\n",
    "\n",
    "        tweets_df.iloc[i]['Sentences'] = sentiments # insert dictionary of sentence: sentiment value into dataframe\n",
    "\n",
    "        tweets_df.iloc[i]['Timestamp'] = tweets_df.iloc[i]['Timestamp'].to_pydatetime() # convert pandas.tslib.Timestamp object to datetime\n",
    "\n",
    "        # Keyword extraction goes here\n",
    "        filtered_words = blob.noun_phrases\n",
    "        temp = []\n",
    "\n",
    "        for element in filtered_words:\n",
    "            for x in range(len(filtered_words)):\n",
    "                if element != filtered_words[x] and element in filtered_words[x]:\n",
    "                    temp.append(element)\n",
    "        parts_of_speech = blob.tags\n",
    "        for element in temp:\n",
    "            filtered_words = [x for x in filtered_words if x != element]\n",
    "\n",
    "        for x in range(len(parts_of_speech)):\n",
    "            if (parts_of_speech[x])[1] == 'NN':\n",
    "                enter = True\n",
    "                for element in filtered_words:\n",
    "                    if (parts_of_speech[x])[0] in element:\n",
    "                        enter = False\n",
    "                if enter:\n",
    "                    if x > 0 and (parts_of_speech[x - 1])[1] == 'PRP$':\n",
    "                        filtered_words.append((parts_of_speech[x])[0])\n",
    "        parenthesis = []\n",
    "        paren_init = 0\n",
    "        loc_begin = blob.find(\"(\", paren_init)\n",
    "        loc_end = blob.find(\")\", paren_init)\n",
    "\n",
    "        while loc_end >= 0:\n",
    "            parenthesis.append(blob[loc_begin:loc_end])\n",
    "            paren_init = loc_end + 1\n",
    "            loc_begin = blob.find(\"(\", paren_init)\n",
    "            loc_end = blob.find(\")\", paren_init)\n",
    "\n",
    "        for element in filtered_words:\n",
    "            for pelement in parenthesis:\n",
    "                if element in pelement.lower():\n",
    "                    filtered_words = [x for x in filtered_words if x != element]        \n",
    "\n",
    "        for word in filtered_words: # stripping important phrases down to important words\n",
    "            separated = TextBlob(word).words\n",
    "            for j in separated:\n",
    "                j = Word(j.strip())\n",
    "                j = j.singularize().lemmatize() # in case of duplicates singular/plural-wise\n",
    "                tb = ((TextBlob(j).tags)[0])[1]\n",
    "                if j.isalpha() and len(j) > 2 and (tb == 'NN' or tb == 'NNS' or tb == 'VBP'): # filtering out keywords that are too short or not nouns/3rd-person verbs.\n",
    "                    if j in keywords_dict:\n",
    "                        keywords_dict[j][0] += 1\n",
    "                        keywords_dict[j][1] += tweets_df.iloc[i]['Likes']\n",
    "                        keywords_dict[j][2] += tweets_df.iloc[i]['Retweets']\n",
    "                        keywords_dict[j][3] += tweets_df.iloc[i]['Sentiment_Total']\n",
    "                    else:\n",
    "                        keywords_dict[j] = [1, tweets_df.iloc[i]['Likes'], tweets_df.iloc[i]['Retweets'], tweets_df.iloc[i]['Sentiment_Total']]\n",
    "\n",
    "\n",
    "    for key in keywords_dict: # calculating averages of each statistic\n",
    "        keywords_dict[key][1] = int(keywords_dict[key][1] / keywords_dict[key][0])\n",
    "        keywords_dict[key][2] = int(keywords_dict[key][2] / keywords_dict[key][0])\n",
    "        keywords_dict[key][3] = keywords_dict[key][3] / keywords_dict[key][0]\n",
    "        \n",
    "    return keywords_dict\n",
    "\n",
    "\n",
    "def to_dataframe(keywords_dict):\n",
    "    keywords_df = pd.DataFrame.from_dict(keywords_dict, orient='index')\n",
    "    keywords_df.columns = ['Frequency', 'Avg. Likes', 'Avg. Retweets', 'Avg. Sentiment']\n",
    "    keywords_df.index.name = 'Keywords'\n",
    "    keywords_df.reset_index(inplace = True)\n",
    "    keywords_df = keywords_df.sort_values(['Frequency'], ascending = [False], na_position = 'last')\n",
    "    keywords_df = keywords_df[:30]\n",
    "    \n",
    "    return keywords_df\n",
    "\n",
    "\n",
    "def bar_chart1(keywords_df):\n",
    "    data = [go.Bar(\n",
    "            x = list(keywords_df['Keywords']),\n",
    "            y = list(keywords_df['Frequency'])\n",
    "    )]\n",
    "    \n",
    "    layout = go.Layout(\n",
    "        height = 400,\n",
    "        width = 900,\n",
    "        autosize = False,\n",
    "        title='Frequencies Of The Top Keywords In The Last 500 Tweets Made By User',\n",
    "        xaxis=dict(title='Keyword'),\n",
    "        yaxis=dict(title='# of Occurrences')\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "    return py.iplot(fig, filename='basic-bar')\n",
    "\n",
    "\n",
    "def bar_chart2(keywords_df):\n",
    "    data = [go.Bar(\n",
    "            x = list(keywords_df['Keywords']),\n",
    "            y = list(keywords_df['Avg. Likes'])\n",
    "    )]\n",
    "    \n",
    "    layout = go.Layout(\n",
    "        height = 400,\n",
    "        width = 900,\n",
    "        autosize = False,\n",
    "        title='Avg. Likes For The Top 30 Keywords By User',\n",
    "        xaxis=dict(title='Keyword'),\n",
    "        yaxis=dict(title='Average # of Likes')\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "    return py.iplot(fig, filename='basic-bar')\n",
    "\n",
    "\n",
    "def bar_chart3(keywords_df):\n",
    "    data = [go.Bar(\n",
    "            x = list(keywords_df['Keywords']),\n",
    "            y = list(keywords_df['Avg. Sentiment'])\n",
    "    )]\n",
    "    \n",
    "    layout = go.Layout(\n",
    "        height = 400,\n",
    "        width = 900,\n",
    "        autosize = False,\n",
    "        title='Avg. Sentiment For The Top 30 Keywords By User',\n",
    "        xaxis=dict(title='Keyword'),\n",
    "        yaxis=dict(title='Average Sentiment Value')\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "    return py.iplot(fig, filename='basic-bar')\n",
    "\n",
    "def pie_graph(keywords_df):\n",
    "    labels = keywords_df['Keywords'][0:10]\n",
    "    values = keywords_df['Frequency'][0:10]\n",
    "    traces = []\n",
    "\n",
    "    trace = go.Pie(labels = labels, values = values, hoverinfo = 'label+percent+name')\n",
    "    traces.append(trace)\n",
    "\n",
    "    layout = go.Layout(height = 600,\n",
    "                       width = 600,\n",
    "                       autosize = False,\n",
    "                       title = \"User's Most Covered Topics\")\n",
    "    fig = go.Figure(data = traces, layout = layout)\n",
    "    \n",
    "    return py.iplot(fig, show_link = False)\n",
    "\n",
    "def ReadMyMind_main(username):\n",
    "    data_df = tweet_collector(username)\n",
    "    keywords = keyword_data(data_df)\n",
    "    key_df = to_dataframe(keywords)\n",
    "    \n",
    "    return key_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'realDonaldTrump' # working example: President Trump\n",
    "results = ReadMyMind_main(username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Parkkeo1/55.embed\" height=\"400px\" width=\"900px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bar_chart1(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Parkkeo1/55.embed\" height=\"400px\" width=\"900px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bar_chart2(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Parkkeo1/55.embed\" height=\"400px\" width=\"900px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bar_chart3(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Parkkeo1/59.embed\" height=\"600px\" width=\"600px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pie_graph(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
